{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3ff4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryAveragePrecision, BinaryF1Score, BinaryPrecision, BinaryRecall, BinaryHammingDistance\n",
    "\n",
    "# Import model\n",
    "from Transformer_Archs.BiLTSM import BiLSTM\n",
    "\n",
    "# Clear cuda cache\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648ef75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option for 5% or 10% attack data\n",
    "option = '15' \n",
    "\n",
    "# Get data loaders\n",
    "train_dataset = torch.load('./Preprocessed_data/train_dataset_{}.pt'.format(option), weights_only=False)\n",
    "train_config = torch.load('./Preprocessed_data/train_config_{}.pt'.format(option), weights_only=False)\n",
    "train_loader = DataLoader(train_dataset, **train_config)\n",
    "\n",
    "val_dataset = torch.load('./Preprocessed_data/val_dataset_{}.pt'.format(option), weights_only=False)\n",
    "val_config = torch.load('./Preprocessed_data/val_config_{}.pt'.format(option), weights_only=False)\n",
    "val_loader = DataLoader(val_dataset, **val_config)\n",
    "\n",
    "test_dataset = torch.load('./Preprocessed_data/test_dataset_{}.pt'.format(option), weights_only=False)\n",
    "test_config = torch.load('./Preprocessed_data/test_config_{}.pt'.format(option), weights_only=False)\n",
    "test_loader = DataLoader(test_dataset, **test_config)\n",
    "\n",
    "# Set feautures and target size\n",
    "num_features = 86\n",
    "out_features = 10\n",
    "seq_len = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c4f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive dims from a real batch\n",
    "xb, yb = next(iter(train_loader))\n",
    "seq_len      = xb.shape[1]   #\n",
    "in_channels  = xb.shape[2]   # \n",
    "out_features   = yb.shape[1]   # \n",
    "out_channels  = 32                   \n",
    "kernel_size   = 3                  \n",
    "hidden_size   = 64                  \n",
    "lstm_layers   = 2\n",
    "output_size   = out_features\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71093d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM(input_size=num_features, hidden_size=64, num_layers=2, num_labels=out_features).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(x, y):\n",
    "    \"\"\"\n",
    "    Ensures x -> (batch, in_channels=num_features, seq_len) for Conv1d,\n",
    "    and y -> int64 class indices for CrossEntropyLoss.\n",
    "    \"\"\"\n",
    "    # x may be (B, S, F) or (B, F, S)\n",
    "    if x.dim() != 3:\n",
    "        raise ValueError(f\"Expected 3D input (B, S, F) or (B, F, S), got {x.shape}\")\n",
    "\n",
    "    B, A, Bdim = x.shape\n",
    "    if A == seq_len and Bdim == num_features:\n",
    "        # (B, S, F) -> transpose to (B, F, S)\n",
    "        x = x.permute(0, 2, 1)\n",
    "    elif A == num_features and Bdim == seq_len:\n",
    "        # already (B, F, S)\n",
    "        pass\n",
    "    else:\n",
    "        # Try to infer; if not, error out clearly\n",
    "        raise ValueError(f\"Input shape {x.shape} doesn't match either (B,{seq_len},{num_features}) or (B,{num_features},{seq_len}).\")\n",
    "\n",
    "    # Targets: want class indices [0..out_features-1]\n",
    "    if y.dtype != torch.long:\n",
    "        # If one-hot or floats, convert to indices\n",
    "        if y.dim() > 1 and y.size(-1) == out_features:\n",
    "            y = y.argmax(dim=-1)\n",
    "        else:\n",
    "            y = y.long()\n",
    "\n",
    "    return x.to(device), y.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe54e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (simplified)\n",
    "criterion=nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr= 1e-4) #0.001, 5e-4, 1e-3\n",
    "\n",
    "# Define the learning rate scheduler (for example, exponential decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs to wait after last time validation loss improved.\n",
    "            min_delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde888a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchmetrics.classification import (\n",
    "    BinaryAccuracy, BinaryAveragePrecision, BinaryF1Score, BinaryPrecision,\n",
    "    BinaryRecall, BinaryHammingDistance\n",
    ")\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "THR = 0.5 \n",
    "\n",
    "def ensure_channels_first(x, model):\n",
    "    \"\"\"If x is (B,T,C) and conv expects (B,C,T), transpose once.\"\"\"\n",
    "    if hasattr(model, \"conv1\"):\n",
    "        Cexp = model.conv1.in_channels\n",
    "        if x.ndim == 3 and x.shape[1] != Cexp and x.shape[2] == Cexp:\n",
    "            x = x.transpose(1, 2).contiguous()\n",
    "    return x\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_pos_weight(train_loader, device):\n",
    "    # per-class pos_weight for BCEWithLogitsLoss\n",
    "    _, y0 = next(iter(train_loader))\n",
    "    num_labels = y0.shape[1]\n",
    "    pos = torch.zeros(num_labels)\n",
    "    total = 0\n",
    "    for _, yb in train_loader:\n",
    "        pos += yb.sum(dim=0)\n",
    "        total += yb.size(0)\n",
    "    neg = total - pos\n",
    "    eps = 1e-6\n",
    "    return ((neg + eps) / (pos + eps)).to(device), num_labels\n",
    "\n",
    "pos_weight, NUM_LABELS = compute_pos_weight(train_loader, device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17556305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _epoch_pass(loader, train_mode: bool):\n",
    "    model.train(train_mode)\n",
    "\n",
    "    total_loss, n = 0.0, 0\n",
    "    logits_all, targets_all = [], []\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(device); y = y.to(device).float()\n",
    "        x = ensure_channels_first(x, model)\n",
    "\n",
    "        if train_mode:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits = model(x)                          # (B, 10)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "        if train_mode:\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        bs = x.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        n += bs\n",
    "\n",
    "        logits_all.append(logits.detach())\n",
    "        targets_all.append(y.detach())\n",
    "\n",
    "    # --- epoch metrics (Binary, micro over all labels) ---\n",
    "    logits_all = torch.cat(logits_all, dim=0).cpu()\n",
    "    targets_all = torch.cat(targets_all, dim=0).cpu().int()\n",
    "    probs_all = torch.sigmoid(logits_all)\n",
    "\n",
    "    # flatten to treat as one big binary task (“micro”)\n",
    "    p_flat = probs_all.reshape(-1)\n",
    "    t_flat = targets_all.reshape(-1)\n",
    "\n",
    "    acc     = BinaryAccuracy(threshold=THR)(p_flat, t_flat).item()\n",
    "    ap      = BinaryAveragePrecision()(p_flat, t_flat).item()\n",
    "    f1      = BinaryF1Score(threshold=THR)(p_flat, t_flat).item()\n",
    "    prec    = BinaryPrecision(threshold=THR)(p_flat, t_flat).item()\n",
    "    rec     = BinaryRecall(threshold=THR)(p_flat, t_flat).item()\n",
    "    hamming = BinaryHammingDistance(threshold=THR)(p_flat, t_flat).item()\n",
    "\n",
    "    avg_loss = total_loss / max(1, n)\n",
    "\n",
    "    return {\n",
    "        \"loss\": avg_loss, \"acc\": acc, \"ap\": ap, \"f1\": f1,\n",
    "        \"precision\": prec, \"recall\": rec, \"hamming\": hamming\n",
    "    }\n",
    "\n",
    "def train_epoch():\n",
    "    return _epoch_pass(train_loader, train_mode=True)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_epoch():\n",
    "    return _epoch_pass(val_loader, train_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526cdef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping like yours\n",
    "early_stopping = EarlyStopping(patience=10, min_delta=1e-4)\n",
    "\n",
    "train_acc_list, val_acc_list = [], []\n",
    "train_loss_list, val_loss_list = [], []\n",
    "\n",
    "EPOCHS = 150\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr = train_epoch()\n",
    "    va = validate_epoch()\n",
    "\n",
    "    train_acc_list.append(tr[\"acc\"]); val_acc_list.append(va[\"acc\"])\n",
    "    train_loss_list.append(tr[\"loss\"]); val_loss_list.append(va[\"loss\"])\n",
    "\n",
    "    # step scheduler on validation loss (recommended)\n",
    "    scheduler.step(va[\"loss\"])\n",
    "    early_stopping(va[\"loss\"])\n",
    "\n",
    "    # ---- EXACT FORMAT ----\n",
    "    print(f\"Train Epoch: {epoch} - Training Loss: {tr['loss']:.5f} Training accuracy: {tr['acc']*100:.3f}%\")\n",
    "    print(f\"Valid  Epoch: {epoch} - Validation Loss: {va['loss']:.5f} Validation accuracy: {va['acc']*100:.3f}%\")\n",
    "\n",
    "    # (optional) print the rest of the metrics each epoch\n",
    "    # print(f\"  AP: {va['ap']:.6f}  F1: {va['f1']:.6f}  P: {va['precision']:.6f}  R: {va['recall']:.6f}  Hamming: {va['hamming']:.6f}\")\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered. Stopping training.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e80d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_epoch():\n",
    "    # reuse the same micro-binary stats on the test loader\n",
    "    total_loss, n = 0.0, 0\n",
    "    logits_all, targets_all = [], []\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device); y = y.to(device).float()\n",
    "        x = ensure_channels_first(x, model)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "        total_loss += loss.item() * x.size(0); n += x.size(0)\n",
    "        logits_all.append(logits.detach()); targets_all.append(y.detach())\n",
    "\n",
    "    logits_all = torch.cat(logits_all, dim=0).cpu()\n",
    "    targets_all = torch.cat(targets_all, dim=0).cpu().int()\n",
    "    probs_all = torch.sigmoid(logits_all)\n",
    "\n",
    "    p_flat = probs_all.reshape(-1)\n",
    "    t_flat = targets_all.reshape(-1)\n",
    "\n",
    "    acc     = BinaryAccuracy(threshold=THR)(p_flat, t_flat).item()\n",
    "    ap      = BinaryAveragePrecision()(p_flat, t_flat).item()\n",
    "    f1      = BinaryF1Score(threshold=THR)(p_flat, t_flat).item()\n",
    "    prec    = BinaryPrecision(threshold=THR)(p_flat, t_flat).item()\n",
    "    rec     = BinaryRecall(threshold=THR)(p_flat, t_flat).item()\n",
    "    hamming = BinaryHammingDistance(threshold=THR)(p_flat, t_flat).item()\n",
    "\n",
    "    avg_loss = total_loss / max(1, n)\n",
    "    return avg_loss, acc, ap, f1, prec, rec, hamming\n",
    "\n",
    "# ---- Run after training (optionally load best weights first) ----\n",
    "# model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "\n",
    "test_loss, test_acc, ap, f1, prec, rec, ham = test_epoch()\n",
    "print(f\"Test set: Loss: {test_loss:.6f}, Accuracy: {test_acc*100:.3f}%\")\n",
    "print(f\"Average Precision: {ap:.6f}\")\n",
    "print(f\"F1 Score: {f1:.6f}\")\n",
    "print(f\"Precision: {prec:.6f}\")\n",
    "print(f\"Recall: {rec:.6f}\")\n",
    "print(f\"Hamming Distance: {ham:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
