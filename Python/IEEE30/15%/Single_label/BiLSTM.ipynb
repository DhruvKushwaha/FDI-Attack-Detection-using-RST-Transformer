{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3ff4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from Transformer_Archs.Bi_LSTM import BiLSTM\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, average_precision_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648ef75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option for 5% or 10% attack data\n",
    "option = '15' \n",
    "\n",
    "# Get data loaders\n",
    "train_dataset = torch.load('./Preprocessed_data/train_dataset_{}.pt'.format(option), weights_only=False)\n",
    "train_config = torch.load('./Preprocessed_data/train_config_{}.pt'.format(option), weights_only=False)\n",
    "train_loader = DataLoader(train_dataset, **train_config)\n",
    "\n",
    "val_dataset = torch.load('./Preprocessed_data/val_dataset_{}.pt'.format(option), weights_only=False)\n",
    "val_config = torch.load('./Preprocessed_data/val_config_{}.pt'.format(option), weights_only=False)\n",
    "val_loader = DataLoader(val_dataset, **val_config)\n",
    "\n",
    "test_dataset = torch.load('./Preprocessed_data/test_dataset_{}.pt'.format(option), weights_only=False)\n",
    "test_config = torch.load('./Preprocessed_data/test_config_{}.pt'.format(option), weights_only=False)\n",
    "test_loader = DataLoader(test_dataset, **test_config)\n",
    "\n",
    "# Set feautures and target size\n",
    "num_features = 86\n",
    "out_features = 11\n",
    "seq_len = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54850fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_size   = num_features                         \n",
    "hidden_size   = 64                  \n",
    "num_layers   = 2\n",
    "output_size   = out_features\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71093d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM(\n",
    "    input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size= output_size\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(x, y):\n",
    "    \"\"\"\n",
    "    Ensures x -> (batch, in_channels=num_features, seq_len) for Conv1d,\n",
    "    and y -> int64 class indices for CrossEntropyLoss.\n",
    "    \"\"\"\n",
    "    # x may be (B, S, F) or (B, F, S)\n",
    "    if x.dim() != 3:\n",
    "        raise ValueError(f\"Expected 3D input (B, S, F) or (B, F, S), got {x.shape}\")\n",
    "\n",
    "    B, A, Bdim = x.shape\n",
    "    if A == seq_len and Bdim == num_features:\n",
    "        # (B, S, F) -> transpose to (B, F, S)\n",
    "        x = x.permute(0, 2, 1)\n",
    "    elif A == num_features and Bdim == seq_len:\n",
    "        # already (B, F, S)\n",
    "        pass\n",
    "    else:\n",
    "        # Try to infer; if not, error out clearly\n",
    "        raise ValueError(f\"Input shape {x.shape} doesn't match either (B,{seq_len},{num_features}) or (B,{num_features},{seq_len}).\")\n",
    "\n",
    "    # Targets: want class indices [0..out_features-1]\n",
    "    if y.dtype != torch.long:\n",
    "        # If one-hot or floats, convert to indices\n",
    "        if y.dim() > 1 and y.size(-1) == out_features:\n",
    "            y = y.argmax(dim=-1)\n",
    "        else:\n",
    "            y = y.long()\n",
    "\n",
    "    return x.to(device), y.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe54e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs to wait after last time validation loss improved.\n",
    "            min_delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5277d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_val_loss = math.inf\n",
    "patience = 7\n",
    "patience_left = patience\n",
    "best_path = f'.Final_models/Bi-LSTM/Bi_lstm_best_{option}.pt'\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "def run_epoch(loader, train_mode: bool):\n",
    "    if train_mode:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        # Support datasets returning dicts or tuples\n",
    "        if isinstance(batch, dict):\n",
    "            x, y = batch['x'], batch['y']\n",
    "        else:\n",
    "            x, y = batch\n",
    "\n",
    "        x, y = prepare_batch(x, y)\n",
    "\n",
    "        with torch.set_grad_enabled(train_mode):\n",
    "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "                logits = model(x)                 # (B, out_features)\n",
    "                loss = criterion(logits, y)\n",
    "\n",
    "            if train_mode:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item() * x.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    avg_loss = epoch_loss / max(1, total)\n",
    "    acc = correct / max(1, total)\n",
    "    return avg_loss, acc\n",
    "\n",
    "EPOCHS = 50\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_acc = run_epoch(train_loader, train_mode=True)\n",
    "    val_loss, val_acc = run_epoch(val_loader, train_mode=False)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"train loss {train_loss:.4f} acc {train_acc:.3f} | \"\n",
    "          f\"val loss {val_loss:.4f} acc {val_acc:.3f}\")\n",
    "\n",
    "    if val_loss < best_val_loss - 1e-6:\n",
    "        best_val_loss = val_loss\n",
    "        patience_left = patience\n",
    "        torch.save({'model_state': model.state_dict(),\n",
    "                    'config': {\n",
    "                        'in_channels': input_size,\n",
    "                        'hidden_size': hidden_size,\n",
    "                        'num_layers': num_layers,\n",
    "                        'output_size': output_size,\n",
    "                        'seq_length': seq_len\n",
    "                    }}, best_path)\n",
    "        print(f\"  ↳ saved new best model to {best_path}\")\n",
    "    else:\n",
    "        patience_left -= 1\n",
    "        if patience_left == 0:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde888a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "ckpt = torch.load(best_path, map_location=device)\n",
    "model.load_state_dict(ckpt['model_state'])\n",
    "model.eval()\n",
    "\n",
    "test_loss, test_acc = run_epoch(test_loader, train_mode=False)\n",
    "print(f\"TEST | loss {test_loss:.4f} acc {test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2688ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_preds_targets(loader):\n",
    "    model.eval()\n",
    "    preds_all, y_all = [], []\n",
    "    for batch in loader:\n",
    "        if isinstance(batch, dict):\n",
    "            x, y = batch['x'], batch['y']\n",
    "        else:\n",
    "            x, y = batch\n",
    "        x, y = prepare_batch(x, y)\n",
    "        logits = model(x)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        preds_all.append(preds.cpu())\n",
    "        y_all.append(y.cpu())\n",
    "    return torch.cat(preds_all), torch.cat(y_all)\n",
    "\n",
    "preds, targets = collect_preds_targets(test_loader)\n",
    "num_classes = out_features\n",
    "cm = torch.zeros((num_classes, num_classes), dtype=torch.int64)\n",
    "for t, p in zip(targets, preds):\n",
    "    cm[t, p] += 1\n",
    "\n",
    "print(\"Confusion matrix:\\n\", cm.numpy())\n",
    "per_class_acc = (cm.diag() / cm.sum(dim=1).clamp(min=1)).numpy()\n",
    "print(\"Per-class accuracy:\", per_class_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3285e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, balanced_accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Convert to 1D numpy arrays\n",
    "y_true = targets.cpu().numpy().ravel()\n",
    "y_pred = preds.cpu().numpy().ravel()\n",
    "\n",
    "# class_names = [\"class_0\", \"class_1\", ..., \"class_{num_classes-1}\"]\n",
    "class_names = [f\"class_{i}\" for i in range(num_classes)]\n",
    "\n",
    "# Ensure all classes appear in the report even if missing in y_true or y_pred\n",
    "labels = np.arange(num_classes)\n",
    "\n",
    "print(\"Overall accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"Balanced accuracy:\", balanced_accuracy_score(y_true, y_pred))\n",
    "print(\"\\nClassification report:\\n\")\n",
    "print(classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    labels=labels,\n",
    "    target_names=class_names,   # or omit if you don’t want names\n",
    "    digits=4,\n",
    "    zero_division=0             # avoid warnings for empty precision/recall\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13b344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False positive rate\n",
    "fp = cm.sum(axis=0) - np.diag(cm)\n",
    "fn = cm.sum(axis=1) - np.diag(cm)\n",
    "tp = np.diag(cm)\n",
    "tn = cm.sum() - (fp + fn + tp)\n",
    "\n",
    "fpr = fp/(fp + tn)\n",
    "print(\"False Positive Rate\\n No Attack: {:.3f}%, Bus15: {:.3f}%, Bus18: {:.3f}%, Bus19: {:.3f}%, Bus20: {:.3f}%, Bus21: {:.3f}%, \"\n",
    "\"Bus23: {:.3f}%, Bus24: {:.3f}%, Bus26: {:.3f}%, Bus29: {:.3f}%,Bus30: {:.3f}%\".format(fpr[0]*100,\n",
    "                                                        fpr[1]*100,fpr[2]*100, fpr[3]*100, fpr[4]*100, fpr[5]*100, fpr[6]*100, fpr[7]*100, fpr[8]*100, fpr[9]*100, fpr[10]*100))\n",
    "\n",
    "# False negative rate\n",
    "fnr = fn/(fn + tp)\n",
    "print(\"False Negative Rate\\n No Attack: {:.3f}%, Bus15: {:.3f}%, Bus18: {:.3f}%, Bus19: {:.3f}%, Bus20: {:.3f}%, Bus21: {:.3f}%, \"\n",
    "\"Bus23: {:.3f}%, Bus24: {:.3f}%, Bus26: {:.3f}%, Bus29: {:.3f}%,Bus30: {:.3f}%\".format(fnr[0]*100,\n",
    "                                                        fnr[1]*100,fnr[2]*100, fnr[3]*100, fnr[4]*100, fnr[5]*100, fnr[6]*100, fnr[7]*100, fnr[8]*100, fnr[9]*100, fnr[10]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
