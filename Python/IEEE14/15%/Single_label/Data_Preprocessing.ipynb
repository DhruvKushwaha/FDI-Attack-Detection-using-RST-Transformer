{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option for 5% or 10% attack\n",
    "option = '15'\n",
    "data = pd.read_csv(\"./Datasets/IEEE_case14_onebus_{}%.csv\".format(option))\n",
    "\n",
    "# Display the first few rows to understand the structure of the data\n",
    "data.dropna(inplace=True)\n",
    "#data.drop(columns=[\"Iteration\"], inplace=True)\n",
    "print(data.head())\n",
    "\n",
    "print(np.unique(data[\"Attack_Bus_4\"], return_counts=True))\n",
    "print(np.unique(data[\"Attack_Bus_5\"], return_counts=True))\n",
    "print(np.unique(data[\"Attack_Bus_9\"], return_counts=True))\n",
    "print(np.unique(data[\"Attack_Bus_10\"], return_counts=True))\n",
    "print(np.unique(data[\"Attack_Bus_14\"], return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data\n",
    "feature_cols=data.columns.difference(['Attack_Bus_4','Attack_Bus_5',\n",
    "                                      'Attack_Bus_9','Attack_Bus_10', 'Attack_Bus_14'])\n",
    "features=data[feature_cols]\n",
    "targets=data[['Attack_Bus_4','Attack_Bus_5',\n",
    "                                      'Attack_Bus_9','Attack_Bus_10', 'Attack_Bus_14']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Plot the feature importance\n",
    "# convert to one hot to integer for feature selection\n",
    "temp_target = np.zeros(targets.shape[0])\n",
    "temp = targets.to_numpy()\n",
    "for i in range(temp.shape[0]):\n",
    "    if np.sum(temp[i, :]) == 0:\n",
    "        temp_target[i] = 0\n",
    "    else:\n",
    "        temp_target[i] = np.argmax(temp[i,:]) + 1\n",
    "\n",
    "print(np.unique(temp_target, return_counts=True))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(feature_cols, mutual_info_classif(features, temp_target, random_state=0))\n",
    "plt.xlabel('Mutual Information')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = features.columns\n",
    "\n",
    "# Scale the features\n",
    "scaler=StandardScaler()\n",
    "features_scaled=scaler.fit_transform(features)\n",
    "\n",
    "# Scale the targets (No scaling required for classification targets)\n",
    "scaled_target = temp_target.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the sequences\n",
    "# create sequence for Time series prediction\n",
    "# Convert data_scaled to a NumPy array\n",
    "def to_sequences(seq_size, num_features, out_features, obs1, obs2):\n",
    "    x = np.zeros((len(obs1) - seq_size, seq_size, num_features))\n",
    "    y = np.zeros((len(obs2) - seq_size, out_features))\n",
    "\n",
    "    for i in range(len(obs1) - seq_size):\n",
    "        window = obs1[i:(i + seq_size)]\n",
    "        after_window = obs2[i + seq_size-1, :]\n",
    "        ##after_window = obs2[i + seq_size - out_seq + 1:i + seq_size + 1]\n",
    "        # Create sequence\n",
    "        x[i] = window.reshape(1,-1, num_features)\n",
    "        y[i] = after_window.reshape(-1, out_features)\n",
    "        \n",
    "    return x,y\n",
    "\n",
    "# Create test and train sequence data\n",
    "features = features_scaled\n",
    "target = scaled_target\n",
    "# feature space\n",
    "num_features = features_scaled.shape[1]\n",
    "out_features = scaled_target.shape[1]\n",
    "\n",
    "# Increase magnitude for better prediction\n",
    "amp_increase = 1\n",
    "\n",
    "# Select sequence length\n",
    "sequence_size = 12\n",
    "\n",
    "# Create sequence and increase magnitude\n",
    "# Obs 1 : Features      Obs2 : Target\n",
    "X, y = to_sequences(sequence_size, num_features,\n",
    "                    out_features, features_scaled, scaled_target)\n",
    "\n",
    "# Increase magnitude for inputs\n",
    "X = amp_increase*X\n",
    "y = amp_increase*y\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# Change Shape for PyTorch\n",
    "y = y.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 1. train_test_split with stratify\n",
    "X_train1, X_test, y_train1, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y, shuffle=True)\n",
    "print(\"Train set (train_test_split):\", X_train1.shape)\n",
    "print(\"Test set (train_test_split):\", X_test.shape)\n",
    "\n",
    "# Validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train1, y_train1, test_size=0.2, random_state=42, stratify=y_train1, shuffle=True)\n",
    "print(\"Train set (train_test_split):\", X_train.shape)\n",
    "print(\"Validation set (train_test_split):\", X_val.shape)\n",
    "print(\"Train set labels shape:\", y_train.shape)\n",
    "print(\"Unique values in train set:\", np.unique(y_train, return_counts=True))\n",
    "print(\"Unique values in validation set:\", np.unique(y_val, return_counts=True))\n",
    "print(\"Unique values in test set:\", np.unique(y_test, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 128\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 128\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset and configuration parameters\n",
    "torch.save(train_dataset, './Preprocessed_data/train_dataset.pt')\n",
    "train_config = {\n",
    "    'batch_size': batch_size,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 0  # adjust as needed\n",
    "}\n",
    "torch.save(train_config, './Preprocessed_data/train_config.pt')\n",
    "\n",
    "torch.save(val_dataset, './Preprocessed_data/val_dataset.pt')\n",
    "val_config = {\n",
    "    'batch_size': batch_size,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 0  # adjust as needed\n",
    "}\n",
    "torch.save(val_config, './Preprocessed_data/val_config.pt')\n",
    "\n",
    "torch.save(test_dataset, './Preprocessed_data/test_dataset.pt')\n",
    "test_config = {\n",
    "    'batch_size': X_test.shape[0],\n",
    "    'shuffle': False,\n",
    "    'num_workers': 0  # adjust as needed\n",
    "}\n",
    "torch.save(test_config, './Preprocessed_data/test_config.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
