{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option for 5% or 10% attack\n",
    "option = '5'\n",
    "data = pd.read_csv(\"./Datasets/IEEE_case118_multibus_{}%.csv\".format(option, option))\n",
    "\n",
    "# Display the first few rows to understand the structure of the data\n",
    "data.dropna(inplace=True)\n",
    "#data.drop(columns=[\"Iteration\"], inplace=True)\n",
    "print(data.head())\n",
    "\n",
    "print(np.unique(data[\"Attack_Bus_21\"], return_counts=True))\n",
    "print(np.unique(data[\"Attack_Bus_43\"], return_counts=True))\n",
    "print(np.unique(data[\"Attack_Bus_20\"], return_counts=True))\n",
    "print(np.unique(data[\"Attack_Bus_44\"], return_counts=True))\n",
    "print(np.unique(data[\"Attack_Bus_52\"], return_counts=True))\n",
    "print(np.unique(data[\"Attack_Bus_22\"], return_counts=True))\n",
    "print(np.unique(data[\"Attack_Bus_51\"], return_counts=True))\n",
    "print(np.unique(data[\"Attack_Bus_53\"], return_counts=True))\n",
    "print(np.unique(data[\"Attack_Bus_45\"], return_counts=True))\n",
    "print(np.unique(data[\"Attack_Bus_58\"], return_counts=True))\n",
    "print(np.unique(data[\"Attack_Bus_108\"], return_counts=True))\n",
    "print(np.unique(data[\"Attack_Bus_117\"], return_counts=True))\n",
    "print(np.unique(data[\"Attack_Bus_109\"], return_counts=True))\n",
    "print(np.unique(data[\"Attack_Bus_86\"], return_counts=True))\n",
    "print(np.unique(data[\"Attack_Bus_33\"], return_counts=True))\n",
    "print(np.unique(data[\"Attack_Bus_57\"], return_counts=True))\n",
    "print(np.unique(data[\"Attack_Bus_95\"], return_counts=True))\n",
    "print(np.unique(data[\"Attack_Bus_13\"], return_counts=True))\n",
    "print(np.unique(data[\"Attack_Bus_19\"], return_counts=True))\n",
    "print(np.unique(data[\"Attack_Bus_87\"], return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns which have only one unique value\n",
    "for col in data.columns:\n",
    "    if data[col].nunique() == 1:\n",
    "        data.drop(columns=[col], inplace=True)\n",
    "        print(f\"Dropped column: {col}\")\n",
    "        print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data\n",
    "feature_cols=data.columns.difference(['Attack_Bus_21','Attack_Bus_43','Attack_Bus_20',\n",
    "                                      'Attack_Bus_44','Attack_Bus_52','Attack_Bus_22',\n",
    "                                      'Attack_Bus_51','Attack_Bus_53','Attack_Bus_45',\n",
    "                                      'Attack_Bus_58','Attack_Bus_108','Attack_Bus_117',\n",
    "                                      'Attack_Bus_109','Attack_Bus_86','Attack_Bus_33',\n",
    "                                      'Attack_Bus_57','Attack_Bus_95','Attack_Bus_13',\n",
    "                                      'Attack_Bus_19','Attack_Bus_87'])\n",
    "features=data[feature_cols]\n",
    "targets=data[['Attack_Bus_21','Attack_Bus_43','Attack_Bus_20',\n",
    "                                      'Attack_Bus_44','Attack_Bus_52','Attack_Bus_22',\n",
    "                                      'Attack_Bus_51','Attack_Bus_53','Attack_Bus_45',\n",
    "                                      'Attack_Bus_58','Attack_Bus_108','Attack_Bus_117',\n",
    "                                      'Attack_Bus_109','Attack_Bus_86','Attack_Bus_33',\n",
    "                                      'Attack_Bus_57','Attack_Bus_95','Attack_Bus_13',\n",
    "                                      'Attack_Bus_19','Attack_Bus_87']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler=StandardScaler()\n",
    "features_scaled=scaler.fit_transform(features)\n",
    "\n",
    "# Scale the targets (No scaling required for classification targets)\n",
    "scaled_target = targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the sequences\n",
    "# create sequence for Time series prediction\n",
    "# Convert data_scaled to a NumPy array\n",
    "def to_sequences(seq_size, num_features, out_features, obs1, obs2):\n",
    "    x = np.zeros((len(obs1) - seq_size, seq_size, num_features))\n",
    "    y = np.zeros((len(obs2) - seq_size, out_features))\n",
    "\n",
    "    for i in range(len(obs1) - seq_size):\n",
    "        window = obs1[i:(i + seq_size)]\n",
    "        after_window = obs2[i + seq_size - 1, :]\n",
    "        ##after_window = obs2[i + seq_size - out_seq + 1:i + seq_size + 1]\n",
    "        # Create sequence\n",
    "        x[i] = window.reshape(1,-1, num_features)\n",
    "        y[i] = after_window.reshape(-1, out_features)\n",
    "        \n",
    "    return x,y\n",
    "\n",
    "# Create test and train sequence data\n",
    "features = features_scaled\n",
    "target = scaled_target\n",
    "# feature space\n",
    "num_features = features_scaled.shape[1]\n",
    "out_features = scaled_target.shape[1]\n",
    "\n",
    "# Increase magnitude for better prediction\n",
    "amp_increase = 1\n",
    "\n",
    "# Select sequence length\n",
    "sequence_size = 12\n",
    "\n",
    "# Create sequence and increase magnitude\n",
    "# Obs 1 : Features      Obs2 : Target\n",
    "X, y = to_sequences(sequence_size, num_features,\n",
    "                    out_features, features_scaled, scaled_target.to_numpy())\n",
    "\n",
    "# Increase magnitude for inputs\n",
    "X = amp_increase*X\n",
    "y = amp_increase*y\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "# 1. train_test_split with stratify\n",
    "X_train1, y_train1, X_test, y_test = iterative_train_test_split(X, y, test_size=0.2)\n",
    "print(\"Train set (train_test_split):\", X_train1.shape)\n",
    "print(\"Test set (train_test_split):\", X_test.shape)\n",
    "\n",
    "# Validation split\n",
    "X_train, y_train, X_val, y_val = iterative_train_test_split(X_train1, y_train1, test_size=0.2)\n",
    "print(\"Train set (train_test_split):\", X_train.shape)\n",
    "print(\"Validation set (train_test_split):\", X_val.shape)\n",
    "print(\"Train set labels shape:\", y_train.shape)\n",
    "print(\"Unique values in train set:\", np.unique(y_train, return_counts=True))\n",
    "print(\"Unique values in validation set:\", np.unique(y_val, return_counts=True))\n",
    "print(\"Unique values in test set:\", np.unique(y_test, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 128\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset and configuration parameters\n",
    "torch.save(train_dataset, './Preprocessed_data/train_dataset_{}.pt'.format(option))\n",
    "train_config = {\n",
    "    'batch_size': batch_size,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 0  # adjust as needed\n",
    "}\n",
    "torch.save(train_config, './Preprocessed_data/train_config_{}.pt'.format(option))\n",
    "\n",
    "torch.save(val_dataset, './Preprocessed_data/val_dataset_{}.pt'.format(option))\n",
    "val_config = {\n",
    "    'batch_size': batch_size,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 0  # adjust as needed\n",
    "}\n",
    "torch.save(val_config, './Preprocessed_data/val_config_{}.pt'.format(option))\n",
    "\n",
    "torch.save(test_dataset, './Preprocessed_data/test_dataset_{}.pt'.format(option))\n",
    "test_config = {\n",
    "    'batch_size': X_test.shape[0],\n",
    "    'shuffle': False,\n",
    "    'num_workers': 0  # adjust as needed\n",
    "}\n",
    "torch.save(test_config, './Preprocessed_data/test_config_{}.pt'.format(option))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
